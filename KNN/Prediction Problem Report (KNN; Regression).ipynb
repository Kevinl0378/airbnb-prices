{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (KNN; Regression)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bfff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LogisticRegressionCV, ElasticNetCV, LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import r2_score, accuracy_score, recall_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, \\\n",
    "cross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn import impute\n",
    "import ast\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kevin/Downloads/Northwestern University/Data Science/STAT_303-3/Prediction Problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Step 0) Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_regression.csv')\n",
    "test = pd.read_csv('test_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c359ba5",
   "metadata": {},
   "source": [
    "## Step 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0b0b1",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f921f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns\n",
    "\n",
    "\n",
    "first_ten = train.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_location', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['id', 'host_id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_since']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = train.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 107].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = train.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text', 'price']\n",
    "\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "cleaned_third['price'] = cleaned_third['price'].str.replace('[$,]', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = train.iloc[:, 30:40]\n",
    "fourth_ten.dtypes\n",
    "\n",
    "# Removing: ['first_review']\n",
    "# Converting: ['has_availability']\n",
    "\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review'])\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = train.iloc[:, 40:50]\n",
    "fifth_ten\n",
    "\n",
    "# Removing: ['last_review']\n",
    "# Converting: ['instant_bookable']\n",
    "\n",
    "cleaned_fifth = fifth_ten.drop(columns=['last_review'])\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "last_four = train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df77ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned datasets\n",
    "\n",
    "cleaned_train = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, cleaned_fifth, last_four], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611499c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ade702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values of dummy variables using mode\n",
    "\n",
    "cleaned_train['host_is_superhost'].fillna(cleaned_train['host_is_superhost'].mode()[0], inplace=True)\n",
    "cleaned_train['host_response_time'].fillna(cleaned_train['host_response_time'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e8817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_train_imputed = knn_imputer.fit_transform(cleaned_train[columns_with_missing])\n",
    "cleaned_train_imputed_df = pd.DataFrame(cleaned_train_imputed, columns=columns_with_missing)\n",
    "cleaned_train[columns_with_missing] = cleaned_train_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1568ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = ['review_scores_communication','review_scores_cleanliness', 'number_of_reviews_l30d', \\\n",
    "                                'review_scores_accuracy', 'review_scores_value','review_scores_location', \\\n",
    "                                'review_scores_checkin', 'minimum_minimum_nights', 'maximum_minimum_nights', \\\n",
    "                                'minimum_maximum_nights', 'maximum_maximum_nights', 'availability_60', \\\n",
    "                                'availability_90', 'availability_365','calculated_host_listings_count',\n",
    "                                'calculated_host_listings_count_entire_homes', 'host_listings_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d58d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(cleaned_train.price)\n",
    "X_train = cleaned_train.drop(columns = 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d67589ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_redundant = X_train.copy()\n",
    "X_train_non_redundant.drop(columns = to_be_removed, inplace = True)\n",
    "X_train_non_redundant = pd.get_dummies(X_train_non_redundant, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8021ca0",
   "metadata": {},
   "source": [
    "#### <font color = 'black'>PolynomialFeatures</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eda0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, interaction_only = True, include_bias = False)\n",
    "X_train_poly = poly.fit_transform(X_train_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d53389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_scaled_poly_df = pd.DataFrame(X_train_poly, columns = poly.get_feature_names_out(X_train_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8c117",
   "metadata": {},
   "source": [
    "#### <font color = 'black'>Scaling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe0913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_poly)\n",
    "X_train_scaled = scaler.transform(X_train_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73228fe",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing test data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e93ab564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the test data\n",
    "\n",
    "first_ten = test.iloc[:, :10]\n",
    "first_ten\n",
    "\n",
    "# Removing: ['host_location', 'host_neighbourhood']\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost']\n",
    "\n",
    "cleaned_ten = first_ten.drop(columns=['host_location', 'host_neighbourhood'])\n",
    "\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = test.iloc[:, 10:20]\n",
    "\n",
    "# Consider removing: []\n",
    "# Consider converting: ['host_has_profile_pic', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 64].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = test.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text']\n",
    "\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = test.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "# Converting: ['has_availability']\n",
    "\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review', 'last_review'])\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = test.iloc[:, 40:50]\n",
    "\n",
    "# Consider removing: []\n",
    "# Consider converting: ['instant_bookable']\n",
    "\n",
    "fifth_ten['instant_bookable'] = fifth_ten['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "last_three = test.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "595faf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the test datasets\n",
    "cleaned_test = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, fifth_ten, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52ce2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ct = cleaned_test.copy()\n",
    "\n",
    "copy_ct['host_is_superhost'].fillna(copy_ct['host_is_superhost'].mode()[0], inplace=True)\n",
    "copy_ct['host_response_time'].fillna(copy_ct['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin']\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "copy_ct_imputed = knn_imputer.fit_transform(copy_ct[columns_with_missing])\n",
    "copy_ct_imputed_df = pd.DataFrame(copy_ct_imputed, columns=columns_with_missing)\n",
    "copy_ct[columns_with_missing] = copy_ct_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e566fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_redundant = copy_ct.drop(columns = to_be_removed)\n",
    "X_test_non_redundant = pd.get_dummies(X_test_non_redundant, drop_first = True)\n",
    "X_test_non_redundant = X_test_non_redundant.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb53249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_test = PolynomialFeatures(2, include_bias = False)\n",
    "poly_test.fit(X_test_non_redundant)\n",
    "X_test_poly = poly_test.transform(X_test_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbe91907",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_scaled_poly_df = pd.DataFrame(X_test_poly, columns = poly_test.get_feature_names_out(X_test_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868d2db",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc34beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_test_scaled = sc.fit_transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## Step 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33d51",
   "metadata": {},
   "source": [
    "It took me three attempts to tune the model hyperparameters. The first attempt was performed without any variable interactions, so the resulting RMSE of the model was quite far from the 130 threshold on Kaggle. My second attempt to tune the hyperparameters came after I utilized `PolynomialFeatures` to add variable interactions to the model, which resulted in a lower RMSE, but just barely over the threshold. After looking over my code, I discovered that I was not scaling the datasets properly, so my third and final attempt was performed after the scaling issues were resolved. This attempt resulted in a RMSE under the 130 threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": [
    "I used grid search (`GridSearchCV`) to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": [
    "The challenge that I encountered was determining the appropriate place to scale my datasets before tuning the hyperparameters. At first, I only scaled my training datatset once, which took place before the process of selecting predictors and tuning the hyperparameters. However, since this resulted in a RMSE that was above the threshold, I decided to scale the training data before using Lasso to eliminate predictors, as well as every time I indexed a set of predictors to perform grid search (in the section **Tuning the Hyperparameters**). By addressing this challenge, I was able to reach the target RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": [
    "It took about 1.5 hours / 90 minutes for the code to tune the hyperparameters. The variable selection process with Lasso lasted roughly one hour, and the GridSearchCV search method took about 30 minutes to finish. I spent about 2 hours on the code that performs the hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23e187",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>Lasso for variable selection</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f3451b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=array([1.00000000e-01, 9.54771611e-02, 9.11588830e-02, 8.70359136e-02,\n",
       "       8.30994195e-02, 7.93409667e-02, 7.57525026e-02, 7.23263390e-02,\n",
       "       6.90551352e-02, 6.59318827e-02, 6.29498899e-02, 6.01027678e-02,\n",
       "       5.73844165e-02, 5.47890118e-02, 5.23109931e-02, 4.99450512e-02,\n",
       "       4.76861170e-02, 4.55293507e-02, 4.34701316e-02, 4.15040476e-02,\n",
       "       3.96268864e-02, 3.78346262e-0...\n",
       "       2.89942285e-05, 2.76828663e-05, 2.64308149e-05, 2.52353917e-05,\n",
       "       2.40940356e-05, 2.30043012e-05, 2.19638537e-05, 2.09704640e-05,\n",
       "       2.00220037e-05, 1.91164408e-05, 1.82518349e-05, 1.74263339e-05,\n",
       "       1.66381689e-05, 1.58856513e-05, 1.51671689e-05, 1.44811823e-05,\n",
       "       1.38262217e-05, 1.32008840e-05, 1.26038293e-05, 1.20337784e-05,\n",
       "       1.14895100e-05, 1.09698580e-05, 1.04737090e-05, 1.00000000e-05]),\n",
       "        cv=10, max_iter=10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.logspace(-1,-5,200)\n",
    "lassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 10000)\n",
    "lassocv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "343d93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = lassocv.alpha_)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "coefficients = {}\n",
    "for i in range(len(lasso.coef_)):\n",
    "    coefficients[poly.get_feature_names_out()[i]] = lasso.coef_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb30b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/35/_36mcwyx2kv5hgjlm3qr6sym0000gn/T/ipykernel_3115/3298351333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_predictors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_predictors = pd.Series(coefficients).sort_values(key = abs, ascending = False).index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded74c08",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Tuning the Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "103bfa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = pd.DataFrame(columns = ['r', 'predictors', 'n_neighbors', 'weights', 'p', 'Optimal RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dffdd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20, 51):\n",
    "    predictors = sorted_predictors[:i]\n",
    "    \n",
    "    X = X_train_non_scaled_poly_df.loc[:, predictors]\n",
    "    sc  = StandardScaler()\n",
    "    Xstd = sc.fit_transform(X)\n",
    "    \n",
    "    # Using GridSearchCV to tune the hyperparameters:\n",
    "\n",
    "    # 1) Create the model\n",
    "    model = KNeighborsRegressor(metric = 'minkowski')\n",
    "\n",
    "    # 2) Create a hyperparameter grid (as a dict)   \n",
    "    grid = {'n_neighbors': np.arange(1, 21), 'weights':['uniform', 'distance'], 'p': [1, 1.1]}\n",
    "\n",
    "    # 3) Create the Kfold object\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "    # 4) Create the CV object\n",
    "    gcv = GridSearchCV(model, param_grid = grid, cv = kfold, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "    # Fit the models, and cross-validate\n",
    "    gcv.fit(Xstd, y_train)    \n",
    "    analysis_results = analysis_results.append({'r': i, 'predictors': predictors, 'n_neighbors': gcv.best_params_['n_neighbors'], 'weights': gcv.best_params_['weights'], 'p': gcv.best_params_['p'], 'Optimal RMSE': -gcv.best_score_}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc357d",
   "metadata": {},
   "source": [
    "### Optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33497b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>predictors</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>p</th>\n",
       "      <th>Optimal RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39</td>\n",
       "      <td>[num_bathrooms property_category_Room, num_bat...</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     r                                         predictors n_neighbors  \\\n",
       "19  39  [num_bathrooms property_category_Room, num_bat...          11   \n",
       "\n",
       "     weights  p  Optimal RMSE  \n",
       "19  distance  1       0.42027  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_results.sort_values(by = 'Optimal RMSE').iloc[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba9abc",
   "metadata": {},
   "source": [
    "The optimal value of `n_neighbors` is 11, the optimal number of predictors is 39, the optimal `weights` is `distance`, and the optimal value of `p`is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## Step 3) Developing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_predictors is defined in Step 2)\n",
    "predictors = sorted_predictors[:39]\n",
    "Xtrain = X_train_non_scaled_poly_df.loc[:, predictors]\n",
    "Xtest = X_test_non_scaled_poly_df.loc[:, predictors]\n",
    "sc  = StandardScaler()\n",
    "Xtrain_std = sc.fit_transform(Xtrain)\n",
    "Xtest_std = sc.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38749ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors = 11, metric = 'minkowski', weights = 'distance', p = 1).fit(Xtrain_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## Step 4) Ad-hoc steps for further improving model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d07599c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.exp(model.predict(Xtest_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## Step 5) Exporting the predictions in the format required to submit on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ct.insert(1, \"predicted\", test_pred)\n",
    "to_submit = copy_ct.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a7e1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_submit.to_csv('KNN Regression - Final Submission.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
