{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (Ensemble; Regression)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d649cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, LogisticRegressionCV, LogisticRegression, ElasticNet\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate, \\\n",
    "GridSearchCV, RandomizedSearchCV, ParameterGrid, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier, \\\n",
    "VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostRegressor,AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, \\\n",
    "accuracy_score, precision_score, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_objective, plot_histogram, plot_convergence\n",
    "\n",
    "from sklearn import impute\n",
    "import ast\n",
    "import itertools as it\n",
    "from sklearn.tree import export_graphviz \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kevin/Downloads/Northwestern University/Data Science/STAT_303-3/Prediction Problems/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Step 0) Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_regression.csv')\n",
    "test = pd.read_csv('test_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "## Step 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ddb3d",
   "metadata": {},
   "source": [
    "### <font color = 'blue'>Pre-processing training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d6fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns\n",
    "\n",
    "\n",
    "first_ten = train.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_location', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['id', 'host_id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_since']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = train.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 107].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = train.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text', 'price']\n",
    "\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "cleaned_third['price'] = cleaned_third['price'].str.replace('[$,]', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = train.iloc[:, 30:40]\n",
    "fourth_ten.dtypes\n",
    "\n",
    "# Removing: ['first_review']\n",
    "# Converting: ['has_availability']\n",
    "\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review'])\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = train.iloc[:, 40:50]\n",
    "fifth_ten\n",
    "\n",
    "# Removing: ['last_review']\n",
    "# Converting: ['instant_bookable']\n",
    "\n",
    "cleaned_fifth = fifth_ten.drop(columns=['last_review'])\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "last_four = train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebe9e7",
   "metadata": {},
   "source": [
    "#### <font color = blue>Imputing Missing Values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a0c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned datasets\n",
    "\n",
    "cleaned_train = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, cleaned_fifth, last_four], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e702d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb65b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values of dummy variables using mode\n",
    "\n",
    "cleaned_train['host_is_superhost'].fillna(cleaned_train['host_is_superhost'].mode()[0], inplace=True)\n",
    "cleaned_train['host_response_time'].fillna(cleaned_train['host_response_time'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d371472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_train_imputed = knn_imputer.fit_transform(cleaned_train[columns_with_missing])\n",
    "cleaned_train_imputed_df = pd.DataFrame(cleaned_train_imputed, columns=columns_with_missing)\n",
    "cleaned_train[columns_with_missing] = cleaned_train_imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb056af",
   "metadata": {},
   "source": [
    "#### <font color=blue>Creating predictors and the response variables</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f18aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(cleaned_train.price)\n",
    "X_train = cleaned_train.drop(columns = 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a66b8",
   "metadata": {},
   "source": [
    "#### <font color = blue>Transforming predictors</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32efbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_logged = ['reviews_per_month','accommodates','beds', 'host_total_listings_count', 'minimum_nights', 'maximum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd6d754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_logged_zero = ['number_of_reviews_ltm', 'number_of_reviews', 'num_bathrooms', \\\n",
    "                     'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05561eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in to_be_logged:\n",
    "    X_train[column] = np.log(X_train[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e669f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in to_be_logged_zero:\n",
    "    X_train[column] = np.log(1 + X_train[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f725b",
   "metadata": {},
   "source": [
    "#### <font color = blue>One-hot encoding categorical predictors and applying PolynomialFeatures</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6738a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot = pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c123c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 1\n",
    "for x in list(X_train_onehot.isnull().sum().sort_values().values):\n",
    "    if x != 0:\n",
    "        raise Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604df14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 2\n",
    "for x in list((X_train_onehot == -np.inf).sum().sort_values().values):\n",
    "    if x != 0:\n",
    "        raise Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11b25d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_redundant = PolynomialFeatures(2, include_bias = False)\n",
    "X_train_redundant_poly = poly_redundant.fit_transform(X_train_onehot)\n",
    "X_train_redundant_poly_df = pd.DataFrame(X_train_redundant_poly, columns = poly_redundant.get_feature_names_out(X_train_onehot.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1431f",
   "metadata": {},
   "source": [
    "### <font color = 'green'>Pre-processing test data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a6c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the test data\n",
    "\n",
    "first_ten = test.iloc[:, :10]\n",
    "first_ten\n",
    "\n",
    "# Removing: ['host_location', 'host_neighbourhood']\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost']\n",
    "\n",
    "cleaned_ten = first_ten.drop(columns=['host_location', 'host_neighbourhood'])\n",
    "\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = test.iloc[:, 10:20]\n",
    "\n",
    "# Consider removing: []\n",
    "# Consider converting: ['host_has_profile_pic', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 64].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = test.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text']\n",
    "\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = test.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "# Converting: ['has_availability']\n",
    "\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review', 'last_review'])\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = test.iloc[:, 40:50]\n",
    "\n",
    "# Consider removing: []\n",
    "# Consider converting: ['instant_bookable']\n",
    "\n",
    "fifth_ten['instant_bookable'] = fifth_ten['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "last_three = test.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a6b64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the test datasets\n",
    "cleaned_test = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, fifth_ten, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c2e45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ct = cleaned_test.copy()\n",
    "\n",
    "# Imputing missing values\n",
    "\n",
    "copy_ct['host_is_superhost'].fillna(copy_ct['host_is_superhost'].mode()[0], inplace=True)\n",
    "copy_ct['host_response_time'].fillna(copy_ct['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin', 'number_of_reviews_ltm']\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "copy_ct_imputed = knn_imputer.fit_transform(copy_ct[columns_with_missing])\n",
    "copy_ct_imputed_df = pd.DataFrame(copy_ct_imputed, columns=columns_with_missing)\n",
    "copy_ct[columns_with_missing] = copy_ct_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "614f9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_logged = ['reviews_per_month','accommodates','beds', 'host_total_listings_count', 'minimum_nights', 'maximum_nights']\n",
    "\n",
    "to_be_logged_zero = ['number_of_reviews_ltm', 'number_of_reviews', 'num_bathrooms', \\\n",
    "                     'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c72b3c",
   "metadata": {},
   "source": [
    "#### <font color = green>Transforming predictors</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7ad6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in to_be_logged:\n",
    "    copy_ct[column] = np.log(copy_ct[column], where = copy_ct[column] > 0)\n",
    "    \n",
    "for column in to_be_logged_zero:\n",
    "    copy_ct[column] = np.log(1 + copy_ct[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a33a1",
   "metadata": {},
   "source": [
    "#### <font color = green>One-hot encoding categorical predictors and applying PolynomialFeatures</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "260c2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_redundant = copy_ct\n",
    "X_test_redundant = pd.get_dummies(X_test_redundant)\n",
    "X_test_redundant = X_test_redundant.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a9ee287",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, include_bias = False)\n",
    "poly.fit(X_test_redundant)\n",
    "X_test_poly = poly.transform(X_test_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a98a6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_scaled_poly_df = pd.DataFrame(X_test_poly, columns = poly.get_feature_names_out(X_test_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## Step 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33d51",
   "metadata": {},
   "source": [
    "I made 30 attempts at tuning the model hyperparameters before I was able to reach a score of under 105 on Kaggle; however, in the end, it was a simple untuned CatBoost that got me below the threshold. During my 30 attempts, I tried a variety of boosting models, as well as several combinations of models ensembled using `StackingRegressor`. However, none of these models or ensembles were able to reach a score of under 105. The critical change that I made to achieve my score was improve my variable selection process by log-transforming predictors during the data pre-processing stage. Once I finished this, I was able to achieve my lowest score at the time by simply using an untuned CatBoost model. To get the model under the 105 threshold, I used trial-and-error to determine the appropriate slope to multiply the predictions made by the untuned CatBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": [
    "As I was tuning the boosting models, I used the `RandomizedSearchCV()` search method because of its computational benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": [
    "The main challenge that I faced when tuning the hyperparameters was the uncertainty of whether my actions were actually beneficial. For example, tuning my CatBoost model sometimes decreased model performance, which was extremely frustrating since the tuning process took a long time to complete and I had maintained the belief that tuning would result in a better performance. As a result, I often found myself uncertain of what the best courses of action were. Eventually, this uncertainty reached a point where I ran out of ideas to pursue and decided to review the pre-processing portion of the prediction problem. This is when I realized that I was not log-transforming any of the predictors, which could have been leading to the selection of a poor set of predictors, thus affecting the performance of my boosting models. After visualizing the distribution of the predictors and log-transforming the ones that seemed skewed, I developed an untuned CatBoost model, which performed the best out of all of my attempts and gave me the encouragement that I needed to reach the 105 threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": [
    "I spent over 70 hours on hyperparameter tuning for the various boosting models that I tried out. From Wednesday, May 29th, through Sunday, June 2nd, my computer was constantly tuning boosting models. I found a method to leave my computer running both overnight and when I was away from my computer in order to finish searching over particularly large grids. By contrast, the hyperparameter tuning process for variable selection using Lasso required less than a minute to complete. In addition, the final model that I used to reach the 105 threshold was an untuned CatBoost model, which took about one minute to code and train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee06ca",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25b37a",
   "metadata": {},
   "source": [
    "#### <font color = red>Variable selection using Lasso</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3d4fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_redundant_poly_df)\n",
    "X_train_scaled = scaler.transform(X_train_redundant_poly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7b28d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken =  0.45 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "alphas = np.logspace(-1,-3,30)\n",
    "lassocv = LassoCV(alphas = alphas, cv = 5, max_iter = 1000)\n",
    "lassocv.fit(X_train_scaled, y_train)\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0716a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = lassocv.alpha_)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "coefficients = {}\n",
    "for i in range(len(lasso.coef_)):\n",
    "    coefficients[poly_redundant.get_feature_names_out()[i]] = lasso.coef_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cdb45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.Series(data = coefficients)\n",
    "non_zero_coefficients = coefficients[coefficients != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fede8",
   "metadata": {},
   "source": [
    "#### Optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33497b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of the lasso regularization hyperparameter is: 0.0016102620275609393\n"
     ]
    }
   ],
   "source": [
    "print('The optimal value of the lasso regularization hyperparameter is:', lassocv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## Step 3) Developing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_without_tuning = CatBoostRegressor(random_state=1, verbose = False).fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## Step 4) Ad-hoc steps for further improving model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d07599c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_pred = np.exp(catboost_without_tuning.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56097512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling up the predictions\n",
    "final_pred = 1.1 * untuned_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## Step 5) Exporting the predictions in the format required to submit on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ct.insert(1, \"predicted\", final_pred)\n",
    "to_submit = copy_ct.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5b6afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_submit.to_csv('Ensembling - Completed.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
