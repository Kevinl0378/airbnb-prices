{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (RF; Regression)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5eb66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, \\\n",
    "cross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, \\\n",
    "accuracy_score, precision_score, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn import impute\n",
    "import ast\n",
    "import itertools as it\n",
    "\n",
    "from sklearn.tree import export_graphviz \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import time as time\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kevin/Downloads/Northwestern University/Data Science/STAT_303-3/Prediction Problems/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Step 0) Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_regression.csv')\n",
    "test = pd.read_csv('test_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "## Step 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47320e0",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66885671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns\n",
    "\n",
    "\n",
    "first_ten = train.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_location', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['id', 'host_id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_since']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = train.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 107].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = train.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text', 'price']\n",
    "\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "cleaned_third['price'] = cleaned_third['price'].str.replace('[$,]', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = train.iloc[:, 30:40]\n",
    "fourth_ten.dtypes\n",
    "\n",
    "# Removing: ['first_review']\n",
    "# Converting: ['has_availability']\n",
    "\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review'])\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = train.iloc[:, 40:50]\n",
    "fifth_ten\n",
    "\n",
    "# Removing: ['last_review']\n",
    "# Converting: ['instant_bookable']\n",
    "\n",
    "cleaned_fifth = fifth_ten.drop(columns=['last_review'])\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "last_four = train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af5de158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned datasets\n",
    "\n",
    "cleaned_train = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, cleaned_fifth, last_four], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a05117ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7aa0a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values of dummy variables using mode\n",
    "\n",
    "cleaned_train['host_is_superhost'].fillna(cleaned_train['host_is_superhost'].mode()[0], inplace=True)\n",
    "cleaned_train['host_response_time'].fillna(cleaned_train['host_response_time'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "885bcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_train_imputed = knn_imputer.fit_transform(cleaned_train[columns_with_missing])\n",
    "cleaned_train_imputed_df = pd.DataFrame(cleaned_train_imputed, columns=columns_with_missing)\n",
    "cleaned_train[columns_with_missing] = cleaned_train_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "672dc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = ['review_scores_communication','review_scores_cleanliness', 'number_of_reviews_l30d', \\\n",
    "                                'review_scores_accuracy', 'review_scores_value','review_scores_location', \\\n",
    "                                'review_scores_checkin', 'minimum_minimum_nights', 'maximum_minimum_nights', \\\n",
    "                                'minimum_maximum_nights', 'maximum_maximum_nights', 'availability_60', \\\n",
    "                                'availability_90', 'availability_365','calculated_host_listings_count',\n",
    "                                'calculated_host_listings_count_entire_homes', 'host_listings_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13492e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(cleaned_train.price)\n",
    "X_train = cleaned_train.drop(columns = 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb666d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_redundant = X_train.copy()\n",
    "X_train_non_redundant.drop(columns = to_be_removed, inplace = True)\n",
    "X_train_non_redundant = pd.get_dummies(X_train_non_redundant, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0de11f",
   "metadata": {},
   "source": [
    "#### <font color = 'black'>PolynomialFeatures</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d013297",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, interaction_only = True, include_bias = False)\n",
    "X_train_poly = poly.fit_transform(X_train_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89ce9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_scaled_poly_df = pd.DataFrame(X_train_poly, columns = poly.get_feature_names_out(X_train_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632123e",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing test data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "25b6ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the test data\n",
    "\n",
    "first_ten = test.iloc[:, :10]\n",
    "first_ten\n",
    "\n",
    "# Removing: ['host_location', 'host_neighbourhood']\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost']\n",
    "\n",
    "cleaned_ten = first_ten.drop(columns=['host_location', 'host_neighbourhood'])\n",
    "\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = test.iloc[:, 10:20]\n",
    "\n",
    "# Consider removing: []\n",
    "# Consider converting: ['host_has_profile_pic', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 64].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = test.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text']\n",
    "\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = test.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "# Converting: ['has_availability']\n",
    "\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review', 'last_review'])\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = test.iloc[:, 40:50]\n",
    "\n",
    "# Consider removing: []\n",
    "# Consider converting: ['instant_bookable']\n",
    "\n",
    "fifth_ten['instant_bookable'] = fifth_ten['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "last_three = test.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78c7d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the test datasets\n",
    "cleaned_test = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, fifth_ten, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f59f9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ct = cleaned_test.copy()\n",
    "\n",
    "copy_ct['host_is_superhost'].fillna(copy_ct['host_is_superhost'].mode()[0], inplace=True)\n",
    "copy_ct['host_response_time'].fillna(copy_ct['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin']\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "copy_ct_imputed = knn_imputer.fit_transform(copy_ct[columns_with_missing])\n",
    "copy_ct_imputed_df = pd.DataFrame(copy_ct_imputed, columns=columns_with_missing)\n",
    "copy_ct[columns_with_missing] = copy_ct_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7601048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_redundant = copy_ct.drop(columns = to_be_removed)\n",
    "X_test_non_redundant = pd.get_dummies(X_test_non_redundant, drop_first = True)\n",
    "X_test_non_redundant = X_test_non_redundant.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4217c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_test = PolynomialFeatures(2, include_bias = False)\n",
    "poly_test.fit(X_test_non_redundant)\n",
    "X_test_poly = poly_test.transform(X_test_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f948d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_scaled_poly_df = pd.DataFrame(X_test_poly, columns = poly_test.get_feature_names_out(X_test_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33d51",
   "metadata": {},
   "source": [
    "The first time that I tuned the model hyperparameters, I was able to achieve a RMSE that was barely under the 128 threshold; this model was tuned with all of the predictors after utilizing PolynomialFeatures with order 2. However, since I wanted to add some cushion to my score, I decided to tune the model using only the most relevant predictors, which were selected using Lasso. On my second attempt, I was able to achieve a RMSE that I was satisfied with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": [
    "I used grid search (`GridSearchCV`) to determine the number of predictors to use in the random forest model. To tune the random forest model, I used a tuning method similar to grid search, where I tested each possible hyperparameter combination using a `for` loop and selected the one that resulted in the lowest RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": [
    "The main challenge that I faced was that the optimal hyperparameter value (`max_features`) for my random forest model sometimes fluctuated, which resulted in slightly different scores on Kaggle (although they were all below the 128 RMSE threshold). In order to eliminate this variation, I decided to use `random_state=1` in both the tuning and training stage of my random forest model; this stabilized both the optimal hyperparameter value and the resulting score on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": [
    "It took about 1 hour for the code to perform the variable selection process. However, tuning the hyperparameters for the random forest model was very quick, as the code finished in under a minute. I spent about 1 hour on the code that performs the hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0b390",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>Lasso for variable selection</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "990cd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_poly)\n",
    "X_train_scaled = scaler.transform(X_train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc896705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032550885998350564"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso for variable selection\n",
    "alphas = np.logspace(-1,-5,200)\n",
    "lassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 1000)\n",
    "lassocv.fit(X_train_scaled, y_train)\n",
    "\n",
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91879e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = lassocv.alpha_)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "coefficients = {}\n",
    "for i in range(len(lasso.coef_)):\n",
    "    coefficients[poly.get_feature_names_out()[i]] = lasso.coef_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "737694e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_predictors = pd.Series(coefficients).sort_values(key = abs, ascending = False).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf800a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = pd.DataFrame(columns = ['r', 'predictors', 'n_neighbors', 'weights', 'p', 'Optimal RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6032db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20, 51):\n",
    "    predictors = sorted_predictors[:i]\n",
    "    \n",
    "    X = X_train_non_scaled_poly_df.loc[:, predictors]\n",
    "    sc  = StandardScaler()\n",
    "    Xstd = sc.fit_transform(X)\n",
    "    \n",
    "    # Using GridSearchCV to tune the hyperparameters:\n",
    "\n",
    "    # 1) Create the model\n",
    "    model = KNeighborsRegressor(metric = 'minkowski')\n",
    "\n",
    "    # 2) Create a hyperparameter grid (as a dict)   \n",
    "    grid = {'n_neighbors': np.arange(1, 21), 'weights':['uniform', 'distance'], 'p': [1, 1.1]}\n",
    "\n",
    "    # 3) Create the Kfold object\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "    # 4) Create the CV object\n",
    "    gcv = GridSearchCV(model, param_grid = grid, cv = kfold, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "    # Fit the models, and cross-validate\n",
    "    gcv.fit(Xstd, y_train)    \n",
    "    analysis_results = analysis_results.append({'r': i, 'predictors': predictors, 'n_neighbors': gcv.best_params_['n_neighbors'], 'weights': gcv.best_params_['weights'], 'p': gcv.best_params_['p'], 'Optimal RMSE': -gcv.best_score_}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90b12274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>predictors</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>p</th>\n",
       "      <th>Optimal RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39</td>\n",
       "      <td>[num_bathrooms property_category_Room, num_bat...</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>0.420202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     r                                         predictors n_neighbors  \\\n",
       "19  39  [num_bathrooms property_category_Room, num_bat...          11   \n",
       "\n",
       "     weights  p  Optimal RMSE  \n",
       "19  distance  1      0.420202  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_results.sort_values(by = 'Optimal RMSE').iloc[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c60e3",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>Tuning the Random Forest model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33497b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =  0.3111697832743327  minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "params = {'n_estimators': [300],\n",
    "          'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "\n",
    "param_list=list(it.product(*(params[Name] for Name in params)))\n",
    "\n",
    "oob_score = [0]*len(param_list)\n",
    "i=0\n",
    "for pr in param_list:\n",
    "    model = RandomForestRegressor(random_state=1, oob_score=True,verbose=False,n_estimators = pr[0], max_features=pr[1], \\\n",
    "                                  n_jobs=-1).fit(X_train_non_scaled_poly_df[sorted_predictors[:39]],y_train)\n",
    "    oob_score[i] = mean_absolute_error(model.oob_prediction_, y_train)\n",
    "    i=i+1\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"time taken = \", (end_time-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca62b7",
   "metadata": {},
   "source": [
    "### Optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b379092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params =  (300, 0.5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params = \", param_list[np.argmin(oob_score)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f92a1",
   "metadata": {},
   "source": [
    "**The optimal value of `max_features` is 0.50 and the optimal number of predictors is 39.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## Step 3) Developing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = RandomForestRegressor(random_state=1, oob_score=True,n_estimators=1000, \\\n",
    "                                    max_features=0.5,n_jobs=-1).fit(X_train_non_scaled_poly_df[sorted_predictors[:39]], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## Step 4) Ad-hoc steps for further improving model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d07599c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.exp(model_tuned.predict(X_test_non_scaled_poly_df[sorted_predictors[:39]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## Step 5) Exporting the predictions in the format required to submit on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ct.insert(1, \"predicted\", test_pred)\n",
    "to_submit = copy_ct.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50ecf5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_submit.to_csv('Random Forest - Final.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
